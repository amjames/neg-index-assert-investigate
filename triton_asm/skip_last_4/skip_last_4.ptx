//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_80
.address_size 64

	// .globl	triton_
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
;
.global .align 1 .b8 assertFunc_0[25] = {95, 99, 97, 108, 108, 95, 119, 105, 116, 104, 95, 102, 114, 97, 109, 101, 115, 95, 114, 101, 109, 111, 118, 101, 100};
.global .align 1 .b8 assertFile_0[38] = {60, 102, 114, 111, 122, 101, 110, 32, 105, 109, 112, 111, 114, 116, 108, 105, 98, 46, 95, 98, 111, 111, 116, 115, 116, 114, 97, 112, 95, 101, 120, 116, 101, 114, 110, 97, 108, 62};
.global .align 1 .b8 assertMessage_0[37] = {105, 110, 100, 101, 120, 32, 111, 117, 116, 32, 111, 102, 32, 98, 111, 117, 110, 100, 115, 58, 32, 48, 32, 60, 61, 32, 116, 109, 112, 51, 32, 60, 32, 50, 48, 52, 56};

.visible .entry triton_(
	.param .u64 triton__param_0,
	.param .u64 triton__param_1,
	.param .u64 triton__param_2,
	.param .u64 triton__param_3,
	.param .u64 triton__param_4,
	.param .u64 triton__param_5,
	.param .u64 triton__param_6,
	.param .u64 triton__param_7,
	.param .u64 triton__param_8,
	.param .u64 triton__param_9,
	.param .u64 triton__param_10,
	.param .u32 triton__param_11
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<54>;
	.reg .b32 	%r<69>;
	.reg .f32 	%f<73>;
	.reg .b64 	%rd<175>;
	.loc	1 24 0
$L__func_begin0:
	.loc	1 24 0

	ld.param.u64 	%rd16, [triton__param_10];
	ld.param.u64 	%rd15, [triton__param_9];
	ld.param.u64 	%rd14, [triton__param_7];
	ld.param.u64 	%rd13, [triton__param_5];
	ld.param.u64 	%rd12, [triton__param_3];
	ld.param.u64 	%rd11, [triton__param_1];
	ld.param.u64 	%rd97, [triton__param_0];
$L__tmp0:
	.loc	1 26 28
	// begin inline asm
	mov.u32 %r6, %ctaid.x;
	// end inline asm
	.loc	1 26 33
	shl.b32 	%r7, %r6, 10;
	ld.param.u64 	%rd98, [triton__param_2];
	.loc	1 27 36
	mov.u32 	%r8, %tid.x;
	shl.b32 	%r9, %r8, 2;
	ld.param.u64 	%rd99, [triton__param_4];
	and.b32  	%r10, %r9, 508;
	.loc	1 27 23
	or.b32  	%r1, %r7, %r10;
	ld.param.u64 	%rd100, [triton__param_6];
	or.b32  	%r4, %r1, 512;
	ld.param.u64 	%rd101, [triton__param_8];
	.loc	1 29 20
	shr.s32 	%r12, %r1, 31;
	shr.u32 	%r13, %r12, 18;
	add.s32 	%r14, %r1, %r13;
	shr.s32 	%r3, %r14, 14;
	add.s32 	%r15, %r4, %r13;
	shr.s32 	%r5, %r15, 14;
	.loc	1 32 30
	mul.wide.s32 	%rd102, %r3, 8;
	add.s64 	%rd18, %rd97, %rd102;
	mul.wide.s32 	%rd103, %r5, 8;
	add.s64 	%rd26, %rd97, %rd103;
	mov.pred 	%p42, -1;
	.loc	1 32 35
	// begin inline asm
	mov.u64 %rd17, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd17 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd19, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd19 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd21, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd21 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd23, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd23 }, [ %rd18 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd25, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd25 }, [ %rd26 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd27, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd27 }, [ %rd26 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd29, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd29 }, [ %rd26 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd31, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd31 }, [ %rd26 + 0 ];
	// end inline asm
	.loc	1 33 30
	add.s64 	%rd34, %rd98, %rd102;
	add.s64 	%rd42, %rd98, %rd103;
	.loc	1 33 35
	// begin inline asm
	mov.u64 %rd33, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd33 }, [ %rd34 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd35, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd35 }, [ %rd34 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd37, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd37 }, [ %rd34 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd39, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd39 }, [ %rd34 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd41, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd41 }, [ %rd42 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd43, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd43 }, [ %rd42 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd45, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd45 }, [ %rd42 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd47, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd47 }, [ %rd42 + 0 ];
	// end inline asm
	.loc	1 34 31
	add.s64 	%rd50, %rd99, %rd102;
	add.s64 	%rd58, %rd99, %rd103;
	.loc	1 34 36
	// begin inline asm
	mov.u64 %rd49, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd49 }, [ %rd50 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd51, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd51 }, [ %rd50 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd53, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd53 }, [ %rd50 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd55, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd55 }, [ %rd50 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd57, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd57 }, [ %rd58 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd59, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd59 }, [ %rd58 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd61, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd61 }, [ %rd58 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd63, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd63 }, [ %rd58 + 0 ];
	// end inline asm
	.loc	1 35 31
	add.s64 	%rd66, %rd100, %rd102;
	add.s64 	%rd74, %rd100, %rd103;
	.loc	1 35 36
	// begin inline asm
	mov.u64 %rd65, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd65 }, [ %rd66 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd67, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd67 }, [ %rd66 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd69, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd69 }, [ %rd66 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd71, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd71 }, [ %rd66 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd73, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd73 }, [ %rd74 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd75, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd75 }, [ %rd74 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd77, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd77 }, [ %rd74 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd79, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd79 }, [ %rd74 + 0 ];
	// end inline asm
	.loc	1 36 31
	add.s64 	%rd82, %rd101, %rd102;
	add.s64 	%rd90, %rd101, %rd103;
	.loc	1 36 36
	// begin inline asm
	mov.u64 %rd81, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd81 }, [ %rd82 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd83, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd83 }, [ %rd82 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd85, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd85 }, [ %rd82 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd87, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd87 }, [ %rd82 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd89, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd89 }, [ %rd90 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd91, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd91 }, [ %rd90 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd93, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd93 }, [ %rd90 + 0 ];
	// end inline asm
	// begin inline asm
	mov.u64 %rd95, 0x0;
	@%p42 ld.global.L1::evict_last.b64 { %rd95 }, [ %rd90 + 0 ];
	// end inline asm
	.loc	1 39 32
	shr.u64 	%rd104, %rd25, 52;
	shr.u64 	%rd105, %rd17, 52;
	and.b64  	%rd106, %rd105, 2048;
	and.b64  	%rd107, %rd104, 2048;
	add.s64 	%rd9, %rd107, %rd25;
	add.s64 	%rd10, %rd106, %rd17;
	.loc	1 40 50
	or.b64  	%rd108, %rd9, %rd10;
	setp.lt.u64 	%p41, %rd108, 2048;
	@%p41 bra 	$L__BB0_2;
	mov.u64 	%rd109, assertMessage_0;
	cvta.global.u64 	%rd110, %rd109;
	mov.u64 	%rd111, assertFile_0;
	cvta.global.u64 	%rd112, %rd111;
	mov.u64 	%rd113, assertFunc_0;
	cvta.global.u64 	%rd114, %rd113;
	mov.b32 	%r16, 843;
	mov.u64 	%rd115, 1;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd112;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r16;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd114;
	.param .b64 param4;
	st.param.b64 	[param4+0], %rd115;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	} // callseq 0
$L__BB0_2:
	.loc	1 0 50
	shl.b32 	%r65, %r5, 14;
	sub.s32 	%r66, %r4, %r65;
	shl.b32 	%r67, %r3, 14;
	sub.s32 	%r68, %r1, %r67;
	.loc	1 41 30
	shl.b64 	%rd128, %rd10, 16;
	add.s64 	%rd129, %rd11, %rd128;
	mul.wide.s32 	%rd130, %r68, 4;
	add.s64 	%rd116, %rd129, %rd130;
	shl.b64 	%rd131, %rd9, 16;
	add.s64 	%rd132, %rd11, %rd131;
	mul.wide.s32 	%rd133, %r66, 4;
	add.s64 	%rd117, %rd132, %rd133;
	.loc	1 41 50
	// begin inline asm
	mov.u32 %r17, 0x0;
	mov.u32 %r18, 0x0;
	mov.u32 %r19, 0x0;
	mov.u32 %r20, 0x0;
	@%p42 ld.global.v4.b32 { %r17, %r18, %r19, %r20 }, [ %rd116 + 0 ];
	// end inline asm
	mov.b32 	%f1, %r17;
	mov.b32 	%f2, %r18;
	mov.b32 	%f3, %r19;
	mov.b32 	%f4, %r20;
	// begin inline asm
	mov.u32 %r21, 0x0;
	mov.u32 %r22, 0x0;
	mov.u32 %r23, 0x0;
	mov.u32 %r24, 0x0;
	@%p42 ld.global.v4.b32 { %r21, %r22, %r23, %r24 }, [ %rd117 + 0 ];
	// end inline asm
	mov.b32 	%f5, %r21;
	mov.b32 	%f6, %r22;
	mov.b32 	%f7, %r23;
	mov.b32 	%f8, %r24;
	.loc	1 44 32
	shr.u64 	%rd134, %rd33, 52;
	and.b64  	%rd135, %rd134, 2048;
	add.s64 	%rd136, %rd135, %rd33;
	shr.u64 	%rd137, %rd41, 52;
	and.b64  	%rd138, %rd137, 2048;
	add.s64 	%rd139, %rd138, %rd41;
	.loc	1 46 30
	shl.b64 	%rd140, %rd136, 16;
	add.s64 	%rd141, %rd12, %rd140;
	add.s64 	%rd118, %rd141, %rd130;
	shl.b64 	%rd142, %rd139, 16;
	add.s64 	%rd143, %rd12, %rd142;
	add.s64 	%rd119, %rd143, %rd133;
	.loc	1 46 50
	// begin inline asm
	mov.u32 %r25, 0x0;
	mov.u32 %r26, 0x0;
	mov.u32 %r27, 0x0;
	mov.u32 %r28, 0x0;
	@%p42 ld.global.v4.b32 { %r25, %r26, %r27, %r28 }, [ %rd118 + 0 ];
	// end inline asm
	mov.b32 	%f9, %r25;
	mov.b32 	%f10, %r26;
	mov.b32 	%f11, %r27;
	mov.b32 	%f12, %r28;
	// begin inline asm
	mov.u32 %r29, 0x0;
	mov.u32 %r30, 0x0;
	mov.u32 %r31, 0x0;
	mov.u32 %r32, 0x0;
	@%p42 ld.global.v4.b32 { %r29, %r30, %r31, %r32 }, [ %rd119 + 0 ];
	// end inline asm
	mov.b32 	%f13, %r29;
	mov.b32 	%f14, %r30;
	mov.b32 	%f15, %r31;
	mov.b32 	%f16, %r32;
	.loc	1 47 19
	add.f32 	%f17, %f1, %f9;
	add.f32 	%f18, %f2, %f10;
	add.f32 	%f19, %f3, %f11;
	add.f32 	%f20, %f4, %f12;
	add.f32 	%f21, %f5, %f13;
	add.f32 	%f22, %f6, %f14;
	add.f32 	%f23, %f7, %f15;
	add.f32 	%f24, %f8, %f16;
	.loc	1 50 35
	shr.u64 	%rd144, %rd49, 52;
	and.b64  	%rd145, %rd144, 2048;
	add.s64 	%rd146, %rd145, %rd49;
	shr.u64 	%rd147, %rd57, 52;
	and.b64  	%rd148, %rd147, 2048;
	add.s64 	%rd149, %rd148, %rd57;
	.loc	1 52 31
	shl.b64 	%rd150, %rd146, 16;
	add.s64 	%rd151, %rd13, %rd150;
	add.s64 	%rd120, %rd151, %rd130;
	shl.b64 	%rd152, %rd149, 16;
	add.s64 	%rd153, %rd13, %rd152;
	add.s64 	%rd121, %rd153, %rd133;
	.loc	1 52 52
	// begin inline asm
	mov.u32 %r33, 0x0;
	mov.u32 %r34, 0x0;
	mov.u32 %r35, 0x0;
	mov.u32 %r36, 0x0;
	@%p42 ld.global.v4.b32 { %r33, %r34, %r35, %r36 }, [ %rd120 + 0 ];
	// end inline asm
	mov.b32 	%f25, %r33;
	mov.b32 	%f26, %r34;
	mov.b32 	%f27, %r35;
	mov.b32 	%f28, %r36;
	// begin inline asm
	mov.u32 %r37, 0x0;
	mov.u32 %r38, 0x0;
	mov.u32 %r39, 0x0;
	mov.u32 %r40, 0x0;
	@%p42 ld.global.v4.b32 { %r37, %r38, %r39, %r40 }, [ %rd121 + 0 ];
	// end inline asm
	mov.b32 	%f29, %r37;
	mov.b32 	%f30, %r38;
	mov.b32 	%f31, %r39;
	mov.b32 	%f32, %r40;
	.loc	1 53 20
	add.f32 	%f33, %f17, %f25;
	add.f32 	%f34, %f18, %f26;
	add.f32 	%f35, %f19, %f27;
	add.f32 	%f36, %f20, %f28;
	add.f32 	%f37, %f21, %f29;
	add.f32 	%f38, %f22, %f30;
	add.f32 	%f39, %f23, %f31;
	add.f32 	%f40, %f24, %f32;
	.loc	1 56 35
	shr.u64 	%rd154, %rd65, 52;
	and.b64  	%rd155, %rd154, 2048;
	add.s64 	%rd156, %rd155, %rd65;
	shr.u64 	%rd157, %rd73, 52;
	and.b64  	%rd158, %rd157, 2048;
	add.s64 	%rd159, %rd158, %rd73;
	.loc	1 58 31
	shl.b64 	%rd160, %rd156, 16;
	add.s64 	%rd161, %rd14, %rd160;
	add.s64 	%rd122, %rd161, %rd130;
	shl.b64 	%rd162, %rd159, 16;
	add.s64 	%rd163, %rd14, %rd162;
	add.s64 	%rd123, %rd163, %rd133;
	.loc	1 58 52
	// begin inline asm
	mov.u32 %r41, 0x0;
	mov.u32 %r42, 0x0;
	mov.u32 %r43, 0x0;
	mov.u32 %r44, 0x0;
	@%p42 ld.global.v4.b32 { %r41, %r42, %r43, %r44 }, [ %rd122 + 0 ];
	// end inline asm
	mov.b32 	%f41, %r41;
	mov.b32 	%f42, %r42;
	mov.b32 	%f43, %r43;
	mov.b32 	%f44, %r44;
	// begin inline asm
	mov.u32 %r45, 0x0;
	mov.u32 %r46, 0x0;
	mov.u32 %r47, 0x0;
	mov.u32 %r48, 0x0;
	@%p42 ld.global.v4.b32 { %r45, %r46, %r47, %r48 }, [ %rd123 + 0 ];
	// end inline asm
	mov.b32 	%f45, %r45;
	mov.b32 	%f46, %r46;
	mov.b32 	%f47, %r47;
	mov.b32 	%f48, %r48;
	.loc	1 59 20
	add.f32 	%f49, %f33, %f41;
	add.f32 	%f50, %f34, %f42;
	add.f32 	%f51, %f35, %f43;
	add.f32 	%f52, %f36, %f44;
	add.f32 	%f53, %f37, %f45;
	add.f32 	%f54, %f38, %f46;
	add.f32 	%f55, %f39, %f47;
	add.f32 	%f56, %f40, %f48;
	.loc	1 62 35
	shr.u64 	%rd164, %rd81, 52;
	and.b64  	%rd165, %rd164, 2048;
	add.s64 	%rd166, %rd165, %rd81;
	shr.u64 	%rd167, %rd89, 52;
	and.b64  	%rd168, %rd167, 2048;
	add.s64 	%rd169, %rd168, %rd89;
	.loc	1 64 31
	shl.b64 	%rd170, %rd166, 16;
	add.s64 	%rd171, %rd15, %rd170;
	add.s64 	%rd124, %rd171, %rd130;
	shl.b64 	%rd172, %rd169, 16;
	add.s64 	%rd173, %rd15, %rd172;
	add.s64 	%rd125, %rd173, %rd133;
	.loc	1 64 52
	// begin inline asm
	mov.u32 %r49, 0x0;
	mov.u32 %r50, 0x0;
	mov.u32 %r51, 0x0;
	mov.u32 %r52, 0x0;
	@%p42 ld.global.v4.b32 { %r49, %r50, %r51, %r52 }, [ %rd124 + 0 ];
	// end inline asm
	mov.b32 	%f57, %r49;
	mov.b32 	%f58, %r50;
	mov.b32 	%f59, %r51;
	mov.b32 	%f60, %r52;
	// begin inline asm
	mov.u32 %r53, 0x0;
	mov.u32 %r54, 0x0;
	mov.u32 %r55, 0x0;
	mov.u32 %r56, 0x0;
	@%p42 ld.global.v4.b32 { %r53, %r54, %r55, %r56 }, [ %rd125 + 0 ];
	// end inline asm
	mov.b32 	%f61, %r53;
	mov.b32 	%f62, %r54;
	mov.b32 	%f63, %r55;
	mov.b32 	%f64, %r56;
	.loc	1 65 20
	add.f32 	%f65, %f49, %f57;
	add.f32 	%f66, %f50, %f58;
	add.f32 	%f67, %f51, %f59;
	add.f32 	%f68, %f52, %f60;
	add.f32 	%f69, %f53, %f61;
	add.f32 	%f70, %f54, %f62;
	add.f32 	%f71, %f55, %f63;
	add.f32 	%f72, %f56, %f64;
	.loc	1 66 25
	mul.wide.s32 	%rd174, %r1, 4;
	add.s64 	%rd126, %rd16, %rd174;
	add.s64 	%rd127, %rd126, 2048;
	.loc	1 66 37
	mov.b32 	%r57, %f65;
	mov.b32 	%r58, %f66;
	mov.b32 	%r59, %f67;
	mov.b32 	%r60, %f68;
	// begin inline asm
	@%p42 st.global.v4.b32 [ %rd126 + 0 ], { %r57, %r58, %r59, %r60 };
	// end inline asm
	mov.b32 	%r61, %f69;
	mov.b32 	%r62, %f70;
	mov.b32 	%r63, %f71;
	mov.b32 	%r64, %f72;
	// begin inline asm
	@%p42 st.global.v4.b32 [ %rd127 + 0 ], { %r61, %r62, %r63, %r64 };
	// end inline asm
	.loc	1 66 4
	ret;
$L__tmp1:
$L__func_end0:

}
	.file	1 "/tmp/torchinductor_paperspace/fy/cfy3vu4f6rdwf2l5qo3jdjpprji3mhuxu5almhljrl5aog4qxkno.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 0
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 126
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 99
.b8 102
.b8 121
.b8 51
.b8 118
.b8 117
.b8 52
.b8 102
.b8 54
.b8 114
.b8 100
.b8 119
.b8 102
.b8 50
.b8 108
.b8 53
.b8 113
.b8 111
.b8 51
.b8 106
.b8 100
.b8 106
.b8 112
.b8 112
.b8 114
.b8 106
.b8 105
.b8 51
.b8 109
.b8 104
.b8 117
.b8 120
.b8 117
.b8 53
.b8 97
.b8 108
.b8 109
.b8 104
.b8 108
.b8 106
.b8 114
.b8 108
.b8 53
.b8 97
.b8 111
.b8 103
.b8 52
.b8 113
.b8 120
.b8 107
.b8 110
.b8 111
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 116
.b8 109
.b8 112
.b8 47
.b8 116
.b8 111
.b8 114
.b8 99
.b8 104
.b8 105
.b8 110
.b8 100
.b8 117
.b8 99
.b8 116
.b8 111
.b8 114
.b8 95
.b8 112
.b8 97
.b8 112
.b8 101
.b8 114
.b8 115
.b8 112
.b8 97
.b8 99
.b8 101
.b8 47
.b8 102
.b8 121
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
	}
	.section	.debug_loc	{	}
